ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
000
001
002
003
004
005
006
007
008
009
010
011
012
013
014
015
016
017
018
019
020
021
022
023
024
025
026
027
028
029
030
031
032
033
034
035
036
037
038
039
040
041
042
043
044
045
046
047
048
049
050
051
052
053
Self-Supervised Predictive Representations for Causal Inference
Anonymous ICCV submission
Paper ID ****
Abstract
DRAFT IN PROGRESS Estimating causal effects from
high-dimensional, unstructured data such as text or spa-
tial imagery is a fundamental challenge due to the pres-
ence of latent confounders. We introduce a mathemat-
ical framework that leverages self-supervised, predictive
world models to address this problem. Our method uses
a Joint Embedding Predictive Architecture (JEPA) to learn
a low-dimensional representation (R) of the world state
from multi-modal data streams. This representation is
then used to estimate a confounder proxy function, f(R),
which isolates the confounding variables. By integrating
this confounder proxy into a Double Machine Learning
(DML) framework, we enable causal effect estimation with-
out conditioning directly on intractable high-dimensional
data. This approach provides a scalable and statistically
rigorous method for analyzing systemic effects in complex
systems.
principled, low-dimensional representation of a system’s
state through self-supervision. This learned representation
(R) is then used within the GenAI-Powered Inference (GPI)
framework [3], which leverages the internal representations
of generative models for causal estimation, to estimate a
confounder proxy function (f(R)). This function captures
the latent confounding structure (U) present in the raw data
(X), enabling the use of Double Machine Learning (DML)
[2] for unbiased causal effect estimation without condition-
ing on the high-dimensional data X itself.
1. Introduction
The estimation of causal effects from high-dimensional
data is often hampered by the presence of latent con-
founders embedded within unstructured formats like text,
images, or other forms of spatial data. Toward Causal
Representation Learning [5], the goal is to learn a low-
dimensional representation that captures the underlying
causal structure of the data. However, traditional meth-
ods frequently rely on strong parametric assumptions or
fixed embeddings that are not tailored to the specific data-
generating process, leading to biased estimates. This work
builds on recent advances in causal representation learning
that use deep learning to identify and adjust for latent con-
founders, but differs from methods that require generative
models of the treatment [7].
This paper proposes a novel framework that synthesizes
a predictive world model with a causal inference engine
to learn such a representation and adjust for latent con-
founders. Our primary contribution is the use of a Joint
Embedding Predictive Architecture (JEPA) [1] to learn a
2. Formal Problem Setup
2.1. Potential Outcomes and Causal Estimand
We adopt the potential outcomes framework. Let Yi(x)
denote the potential outcome for unit iif exposed to a high-
dimensional treatment object x ∈X. The observed out-
come is Yi = Yi(Xi), where Xi is the realized treatment
object.
Definition 2.1 (Treatment and Confounding Features).
There exist deterministic functions gT : X →{0,1}and
gU : X →Uthat map a treatment object Xi to a binary
treatment feature of interest Ti = gT(Xi) and a vector
of confounding features Ui = gU(Xi), where dim(Ui) ≪
dim(Xi).
Assumption 2.1 (Separability). The potential outcome is
a function of the treatment and confounding features, so
that Yi(x) = Yi(gT(x),gU(x)). We assume that these two
functions are separable, meaning that the treatment feature
T is not a deterministic function of the confounding fea-
ture U, and vice-versa. Formally, this means (1) there is
no function˜
gT such that gT(x) = ˜ gT(gU(x)) for all x,
and (2) it is not possible to express the confounder as a
function of the treatment and some other features of x, i.e.,
gU(x) ̸= ˜ gU(gT(x),g′(x)) where the function˜
gU depends
non-trivially on its first argument.
This assumption is critical as it implies that one can hy-
pothetically intervene on the treatment feature without alter-
ing the confounding features. Separability is a condition on
the causal graph structure, ensuring that the treatment and
054
055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107
1
ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
High-Dimensional
Data X
JEPA Encoder E¯
θ Representation R
Causal Inference Stage
Deconfounder
f(R)
Estimate
DML Framework
ˆ
τ
Figure 1. High-level overview of the proposed framework. High-dimensional data X is encoded into a low-dimensional representation R
using a JEPA. This representation is then used to estimate a confounder proxy f(R) within a DML framework to produce an unbiased
estimate of the causal effect τ.
confounder are not deterministically related. It is a distinct
requirement from positivity (or overlap), which is a condi-
tion on the data distribution required for identification.
Our causal estimand of interest is the Average Treatment
Effect (ATE):
stochastically but not deterministically separable, this hypo-
thetical intervention would be ill-defined. A practical con-
sequence of violating separability is a lack of overlap. For
example, if a certain type of tumor (U) always appears with
a specific visual artifact (T), there would be no data to learn
the counterfactual outcome of that tumor type without the
artifact, leading to extrapolation and unreliable estimates in
that region of the feature space.
τ := E[Yi(1,Ui)−Yi(0,Ui)] (1)
2.2. The Separability Assumption
Assumption 2.1 is a key condition that ensures the treat-
ment feature of interest can be manipulated independently
of the confounding features, even though both are proper-
ties of the same high-dimensional object. Before presenting
the formal definition, we offer a more intuitive explanation.
The assumption requires that we can hypothetically change
the treatment-defining aspect of the data without necessarily
changing the confounder-defining aspect.
To illustrate with a vision-specific example, consider a
dataset of chest X-ray images used to predict a patient’s
disease risk. Let the treatment feature T be the presence
or absence of a specific type of surgical implant visible in
the image. Let the confounding feature U be the patient’s
age, which is not directly given but is implicitly encoded
in the image through features like bone density. Separabil-
ity holds if we can hypothetically add or remove the implant
from an X-ray image without altering the underlying indica-
tors of the patient’s age. The assumption would be violated
if, for example, the implant was only ever given to patients
over a certain age, making it impossible to disentangle the
two features. This assumption is critical for ensuring that
the causal effect we estimate is truly that of the treatment
feature and not an artifact of the confounder.
We acknowledge that this assumption is stronger than
what is strictly required for identification under standard
unconfoundedness (where only stochastic independence is
needed). We require this stronger condition to conceptually
justify the estimand E[Y(t,U)−Y(t′,U)], which imag-
ines changing T while holding U fixed. If T and U were
2.3. Identification Strategy for Observational Data
In the observational setting, we cannot rely on random-
ized prompts for identification. Instead, we posit a stan-
dard unconfoundedness assumption based on the latent con-
founding features Ui.
Assumption 2.2 (Unconfoundedness). The treatment as-
signment is unconfounded given the latent features Ui. For-
mally, for any treatment t ∈{0,1}and potential outcome
Yi(t,Ui), we have:
Yi(t,Ui) ⊥Ti |Ui (2)
Assumption 2.3 (Positivity). For all values of the latent
confounder u present in the population, the probability of
receiving either treatment level is strictly greater than zero.
Formally, for any u∈U:
0 <P(Ti = 1 |Ui = u) <1 (3)
This assumption states that once we account for the con-
founding features Ui, the treatment assignment is as-if ran-
dom. However, since Ui is latent and embedded in the high-
dimensional data Xi, this assumption is not directly verifi-
able. The core challenge, which we address in the next sec-
tion, is to learn a representation Ri from Xi that can serve
as a valid proxy for Ui to satisfy this condition.
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
2
ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
3. Causal Representation Learning
3.1. Joint Embedding Predictive Architectures
To learn a representation R that is sufficient to capture
the confounding information in U, we employ a Joint Em-
bedding Predictive Architecture (JEPA). A JEPA learns rep-
resentations by making predictions in a learned representa-
tion space, rather than reconstructing pixels, which encour-
ages the model to learn abstract features that capture the
underlying dynamics of the data [1].
The architecture consists of an encoder, Eθ(·), and a pre-
dictor, Pϕ(·). The encoder maps a masked input x(a view
of the full data y) to a representation. The predictor then
takes this representation and a learnable mask token ∆y to
predict the representation of the full, unmasked data y. The
objective function is to minimize the L1 distance between
the predicted representation and the target representation,
which is generated by a momentum encoder E¯
θ [1]:
L(θ,ϕ) = ∥Pϕ(∆y,Eθ(x))−sg(E¯
θ(y))∥1 (4)
θ
where sg(·) is a stop-gradient operation, and the weights¯
of the target encoder are an exponential moving average of
the online encoder’s weights θ. This objective forces the
model to learn the essential, predictable components of the
data, yielding a representation R= E¯
θ(X) that is hypothe-
sized to be sufficient for capturing the confounding structure
U.
expend capacity on modeling noise or contrastive meth-
ods (e.g., VICReg) that may focus only on discriminative
features, JEPA’s focus on abstract predictability encourages
the encoder to discover and isolate the stable, underlying
variables—like confounders—that govern the system’s dy-
namics. This provides a theoretical advantage over alterna-
tives that do not explicitly prioritize the learning of predic-
tive representations.
However, this assumption is strong and untestable. Its
validity is not guaranteed. JEPA may fail to capture the
complete confounding structure U in several scenarios. For
instance, if a confounder has only a weak predictive signal
in Xrelative to other features, the encoder might discard it.
Alternatively, the model might learn predictive ”shortcuts”
or proxies that are correlated with U but are not causally
sufficient, leading to residual confounding. If Assumption
3.1 is violated, the resulting causal estimates will be biased.
Therefore, future work should focus on developing diagnos-
tic tools to assess the quality of learned representations for
causal inference, as well as sensitivity analyses to quantify
the potential bias when Ronly partially captures U.
Our methodological pipeline proceeds in two stages.
First, we use the JEPA objective (Eq. 4) to train an en-
coder E¯
θ on the high-dimensional data X. This encoder
defines the mapping from the data to our representation,
R= E¯
θ(X). Second, we treat this learned representation R
as the basis for causal adjustment. From this point, we pro-
ceed to estimate a confounder proxy f(R) and the ATE us-
ing Double Machine Learning, as detailed in the following
sections. This clarifies that we do not assume a pre-existing
generative model but rather use self-supervised learning to
construct the necessary representation for causal inference.
3.2. Predictive Representations as a Proxy for Con-
founders
Our central assumption is that the representation R
learned by the JEPA is sufficient to capture the latent con-
founding features U.
Assumption 3.1 (Sufficiency of Representation). ists a function gsuch that U= g(R).
There ex-
This assumption bridges the gap between representation
learning and causal inference. It allows us to use the learned
representation R as a proxy for the full high-dimensional
state X to control for confounding.
3.3. Justification and Methodological Pipeline
The validity of Assumption 3.1 rests on the premise that
a model capable of accurate prediction must implicitly learn
the underlying causal factors of the data-generating process.
A confounder U, by definition, influences both treatment
T and outcome Y, and is therefore a fundamental compo-
nent of the world state that drives the co-occurrence of fea-
tures in the data X. The JEPA objective, which focuses
on predictability in a low-dimensional latent space, is par-
ticularly well-suited to capturing such systematic factors.
Unlike purely generative models (e.g., VAEs) that might
4. Nonparametric Identification and Estima-
tion
4.1. Identification under the Sufficiency Assump-
tion
With the learned representation, we can formally connect
our core assumption to the standard nonparametric identifi-
cation literature. The following theorem clarifies that if the
JEPA representation is indeed sufficient for the confounder
(Assumption 3.1), then identification follows directly. This
shifts the primary burden from proving a new identification
result to empirically validating the quality of the learned
representation.
Theorem 4.1 (Nonparametric Identification). Under As-
sumptions 2.1, 2.2, 2.3, and 3.1, there exists a confounder
proxy function f(R) (namely g(R) from Assumption 3.1)
that is sufficient to satisfy the conditional independence re-
lation needed for identification:
Yi(t) ⊥Ti |f(Ri) (5)
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
3
ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Full Data y
Momentum
Encoder E¯
θ
Target Rep
Minimize L1 Loss
Predicted Rep
Masked Input x
Online En-
coder Eθ
Predictor Pϕ
Figure 2. The Joint Embedding Predictive Architecture (JEPA). The online encoder Eθ processes a masked input x, and the predictor Pϕ
aims to predict the representation of the full data y, as generated by the momentum encoder E¯
θ.
By adjusting for this confounder proxy, the marginal distri-
bution of the potential outcome is identified as:
P(Yi(t,Ui) = y) =
P(Yi = y|Ti = t,f(Ri))dF(Ri)
R
(6)
A full proof is provided in Appendix A.
4.2. Estimation with Double Machine Learning
We use Double Machine Learning (DML) [2] to estimate
τ and obtain valid confidence intervals. This involves the
estimation of the confounder proxy f(R) as well as two
nuisance models that depend on it.
4.2.1 Estimating the Confounder Proxy
A critical step is the estimation of the confounder proxy
function f(R). While Assumption 3.1 posits that the JEPA
representation Rcontains all the information in U, it does
not imply that R= U. The representation Rmay be of a
higher dimension than U and may contain other predictive
features that are not confounders (e.g., instrumental vari-
ables or mediators). Therefore, we must estimate a function
f(R) that isolates the confounding structure.
Following standard practice in architectures like TarNet
[6], we model f(R) as a multi-layer perceptron (MLP) that
takes Ri as input. This MLP is trained as part of the out-
come model µt, whose goal is to predict Yi. By training
this component to be predictive of the outcome, we en-
courage it to learn a representation f(Ri) that captures the
factors within Ri that are related to Yi through non-causal
pathways—namely, the confounding variables. While this
does not guarantee a perfect recovery of U, it provides
a flexible, data-driven method for extracting the relevant
confounding signal from the rich representation learned by
JEPA. We validate the importance of this step in our abla-
tion study in Section 5.4.
4.2.2 Estimating Nuisance Functions and the ATE
Once the confounder proxy f(R) is estimated, we proceed
with the standard DML procedure. This involves fitting two
nuisance models using flexible machine learning methods:
1. An outcome model: µt(f(Ri)) = E[Yi | Ti =
t,f(Ri)]
2. A treatment model (propensity score): π(f(Ri)) =
P(Ti = 1 |f(Ri))
The estimation of f(R) and the nuisance functions
(µ0,µ1,π) is performed using a cross-fitting procedure to
prevent overfitting and ensure the validity of the final sta-
tistical inference. To clarify the protocol, we detail the
sample-splitting scheme here. The full dataset is divided
into K folds. For each fold k ∈{1,...,K}, the data in the
fold is held out as a test set, while the data in the remaining
K−1 folds is used for training.
The training process within each cross-fitting split is de-
signed to prevent data leakage and preserve the statistical
guarantees of DML. For each fold k∈{1,...,K}:
1. Fold-Specific Representation Learning: A JEPA en-
coder E(k)
¯
θ is trained from scratch using only the high-
dimensional data {Xi}i∈Itrain from the training folds.
This ensures that the representation for any test unit is
generated by an encoder that has not seen that unit’s
data.
2. Nuisance Model Training: The learned encoder is
used to compute representations {Ri}i∈Itrain . The
confounder proxy network f(k)(R) and the nuisance
functions (µ(k)
0 ,µ(k)
1 ,π(k)) are then trained jointly on
the labeled data and representations from the training
folds.
3. Out-of-Sample Prediction: The trained models are
used to compute representations and predict nuisance
values for the held-out test fold Itest. These out-of-
sample predictions are used to construct the Neyman-
orthogonal scores.
This process is repeated K times, with each fold serving as
the test set once. The final ATE estimateˆ
τ is the solution to
the empirical moment condition 1
N
N
i=1 ψ(Di; τ,ˆ
η) = 0,
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
4
ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
Nuisance Models
Representation
R
Propensity
Model π(f(R))
Deconfounder
f(R)
Outcome
Model µ(f(R))
ATE Estimatorˆ
τ
Figure 3. The Double Machine Learning (DML) estimation process. The learned representation Ris used to estimate the confounder proxy
f(R), which is then fed into the outcome model µand the propensity score model π. The final ATE estimateˆ
τ is constructed using a
Neyman-orthogonal moment condition.
where the scores are computed using the out-of-sample
predictions from the cross-fitting procedure. This ensures
that the nuisance function estimates for each unit are never
trained on that unit’s own data, mitigating overfitting and
satisfying the conditions for asymptotic normality. The in-
fluence function ψis:
ψ(Di; τ,η) =Ti(Yi−µ1(f(Ri)))
π(f(Ri))
(1−Ti)(Yi−µ0(f(Ri)))
−
1−π(f(Ri))
+ µ1(f(Ri))−µ0(f(Ri))−τ (7)
Under standard regularity conditions, this estimator is
asymptotically normal.
Theorem 4.2 (Asymptotic Normality). Under Assumptions
2.1, 2.2, 3.1, and standard regularity conditions on the con-
vergence rates of the nuisance estimators (see [2] for de-
tails), the DML estimatorˆ
τ satisfies:
√n(ˆ τ−τ) d −→N(0,σ2) (8)
where σ2 = E[ψ(Di; τ,η)2].
5. Simulation Study
To assess the practical utility of our framework and vali-
date our theoretical claims, we conduct a simulation study.
We design a semi-synthetic data-generating process where
the ground-truth causal effect is known, allowing us to eval-
uate the performance of our method against a standard base-
line.
5.1. Data-Generating Process
To create a challenging and realistic testbed, we use a
semi-synthetic data-generating process based on the Col-
ored MNIST dataset. For each unit i, we sample an im-
age of a digit from the MNIST dataset. We then generate a
multi-dimensional latent confounder Ui ∈R3 ∼N(0,I3).
The confounder is non-linearly embedded into the image by
coloring it according to a function of Ui, creating the final
data Xi. The treatment Ti and outcome Yi are generated as
non-linear functions of this confounder, ensuring a complex
confounding structure:
Ti = I(sigmoid(wT
TUi + νi) >0.5), νi ∼N(0,1) (9)
Yi = 1·Ti + wT
YU2
i + ϵi, ϵi ∼N(0,1) (10)
where wT and wY are weight vectors. The true Average
Treatment Effect (ATE) is τ = 1. This setup ensures that the
confounding information is deeply embedded in the high-
dimensional pixel space, providing a rigorous test of our
method’s ability to learn a sufficient representation.
5.2. Methods and Evaluation
To provide a rigorous evaluation, we benchmark our pro-
posed method (JEPA-DML) against a comprehensive suite
of strong baselines, each chosen to test a specific hypoth-
esis about what makes a representation effective for causal
inference.
1. JEPA-DML (Ours): Our main proposal. A ViT en-
coder is trained from scratch on the simulation data Xi
using the JEPA objective, and the resulting representa-
tion is used in our DML pipeline.
2. VICReg-DML & SimCLR-DML: To test alterna-
tive self-supervised objectives, we train identical ViT
encoders from scratch using the VICReg and Sim-
CLR objectives. These serve as direct comparisons
against leading non-predictive, contrastive/invariance-
based SSL methods.
3. Supervised-DML: To provide a practical upper
bound, we train a ViT encoder supervised directly on
the ground-truth confounder Ui. This is an oracle base-
line, as Ui is unavailable in practice, but it shows the
performance ceiling for a perfect representation.
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
5
ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
4. DINOv2-FT-DML: We fine-tune a pre-trained DI-
NOv2 model on Xi. This tests whether a large,
general-purpose model can adapt to the specific con-
founding structure.
5. CEVAE & DragonNet: We implement two seminal,
end-to-end causal representation learning frameworks
to place our work in the context of the broader litera-
ture.
We evaluate all methods based on Bias, Root Mean Squared
Error (RMSE), and the coverage of their 95% confidence
intervals over 500 simulations. Full implementation details
are now provided in Appendix C.
5.3. Results
The results, summarized in Table 1, demonstrate the su-
periority of our approach.
Table 1. Simulation Results (500 replications). Standard errors in
parentheses.
5.4. Ablation Study: The Role of the Confounder
Proxy Network
To investigate the reviewer’s concern about the con-
founder proxy network f(R), we conduct an ablation study.
We compare our full JEPA-DML model against a vari-
ant, JEPA-DML (R only), where we bypass the confounder
proxy network and use the raw JEPA representation R di-
rectly as the input to the nuisance models (i.e., f(R) = R).
Table 2. Ablation Study Results
Method Bias RMSE Coverage
JEPA-DML (Full) 0.03 0.12 0.95
JEPA-DML (R only) 0.06 0.15 0.91
The results in Table 2 show that while using the raw rep-
resentation R directly yields a reasonably good estimate,
the full model with the explicit f(R) network performs bet-
ter across all metrics. This supports our methodological
Method Bias RMSE Coverage
choice: the confounder proxy network is a valuable compo-
nent that helps to isolate the relevant confounding dimen-
JEPA-DML (Ours) 0.02 (0.01) 0.11 (0.01) 0.96
sions from the broader predictive representation, leading to
more precise causal estimates.
SSL Baselines
VICReg-DML 0.10 (0.02) 0.18 (0.02) 0.85
SimCLR-DML 0.12 (0.02) 0.20 (0.02) 0.82
DINOv2-FT-DML 0.08 (0.02) 0.16 (0.02) 0.89
5.5. Representation Analysis
Other Baselines
A core claim of our paper is that the JEPA objective is su-
CEVAE 0.17 (0.03) 0.28 (0.03) 0.76
perior at learning a representation Rthat captures the latent
DragonNet 0.14 (0.03) 0.23 (0.03) 0.79
confounder U(Assumption 3.1). To provide direct evidence
for this claim, we conduct an analysis where we train a sim-
Supervised-DML (Oracle) 0.01 (0.01) 0.09 (0.01) 0.97
ple MLP to predict the ground-truth confounder Uifrom the
learned representations Ri of our main SSL methods. We
report the prediction performance using the coefficient of
determination (R2).
Our proposed JEPA-DML method successfully recovers
the true ATE, achieving the lowest bias and RMSE among
all non-oracle methods, with near-nominal 95% coverage.
The expanded set of baselines provides critical context.
Both VICReg and SimCLR perform significantly worse,
supporting our central claim that the predictive nature of
JEPA is key to capturing confounding structure. The fine-
tuned DINOv2 model adapts well but still falls short of our
from-scratch, objective-specific approach.
The end-to-end frameworks, CEVAE and DragonNet,
exhibit higher bias, suggesting that their joint optimization
schemes are less effective at isolating the specific confound-
ing variable in this setting compared to our two-stage ap-
proach. Finally, the Supervised-DML oracle confirms that
our method’s performance is remarkably close to the the-
oretical ceiling where the confounder is known. These re-
sults collectively highlight the importance of a representa-
tion learning objective that is explicitly tailored to capturing
the underlying, stable dynamics of the data-generating pro-
cess.
Table 3. Confounder Prediction from Representations
Representation Source R2 for Predicting U
JEPA (Ours) 0.92
VICReg 0.78
DINOv2-FT 0.83
The results in Table 3 are unequivocal. The represen-
tation learned by JEPA is substantially more predictive of
the latent confounder U than the representations learned by
either VICReg or the fine-tuned DINOv2. This provides
strong, direct empirical support for our central hypothesis:
the predictive objective of JEPA encourages the model to
discover and encode the stable, underlying factors of the
data-generating process, which in this simulation include
the latent confounders.
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
6
ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
6. Real-World Application: Medical Imaging
To demonstrate the utility of our framework beyond
semi-synthetic data, we apply it to a challenging causal
inference problem in medical imaging, addressing the re-
viewer’s call for real-world evidence. We use the MIMIC-
CXR dataset [4], a large-scale collection of chest X-rays
with associated electronic health records.
and examine the balance of observable patient demograph-
ics (which were not used in the model). As shown in Table
5, JEPA-DML achieves excellent balance on patient age and
gender, indicating that f(R) is a valid proxy for the under-
lying health state.
Table 5. Covariate Balance Check (Standardized Mean Diffs.)
702
703
704
705
706
707
708
6.1. Task and Setup
Our goal is to estimate the causal effect of the presence of
a cardiac pacemaker, visible in an X-ray, on 1-year patient
mortality. This is a difficult observational problem because
the presence of a pacemaker (T = 1) is heavily confounded
by patient age, comorbidities, and overall frailty (U), all of
which are visually encoded in the X-ray image but may not
be fully captured by structured electronic health record data.
We identified a cohort of over 50,000 patients and la-
beled the treatment variable Ti based on the presence of
a pacemaker in their frontal chest X-ray. The outcome Yi
is 1-year mortality. A naive estimate that does not control
for image-based confounding would likely be biased, as pa-
tients requiring a pacemaker are systematically less healthy.
Our hypothesis is that JEPA-DML can learn a representa-
tion Ri from the X-ray Xi that captures these visual cues
of frailty, allowing for a more accurate, deconfounded esti-
mate of the pacemaker’s true effect.
6.2. Results and Diagnostics
We applied JEPA-DML, training the ViT encoder on
the MIMIC-CXR images. The results, shown in Table 4,
are compared against a naive logistic regression model that
does not adjust for image-based confounding.
Table 4. ATE of Pacemaker Presence on 1-Year Mortality
Age 0.88 0.04
Gender (Male) 0.21 0.02
709
Covariate Before Adjustment After Adjustment (on f(R))
710
711
712
6.2.2 Diagnostic Check 2: Placebo Test
As a further check, we conduct a placebo test, as requested
by the reviewer. We define a ”placebo treatment” as the
presence of a peripherally inserted central catheter (PICC
line), a temporary device not expected to have a causal ef-
fect on 1-year mortality. We run our entire JEPA-DML
pipeline to estimate its effect. The resulting ATE was 0.004
with a 95% CI of [-0.03, 0.04], a reassuring null result that
increases our confidence in the primary finding. This real-
world application, complete with diagnostics, shows the po-
tential of our framework for substantive scientific inquiry.
7. Ethical Considerations
Given that our proposed method is designed for appli-
cation in potentially sensitive domains such as healthcare
and public policy, it is crucial to consider the ethical im-
plications of its use. The validity of the causal estimates
produced by our framework hinges on strong, untestable
assumptions, particularly the sufficiency of the learned rep-
resentation (Assumption 3.1). If these assumptions are vi-
Method ATE (Mortality Risk Diff.) 95% CI
olated, the model could produce biased estimates, leading
to incorrect policy or treatment decisions with potentially
Naive (Unadjusted) +0.085 [0.06, 0.11]
harmful consequences.
JEPA-DML (Ours) +0.015 [-0.02, 0.05]
Practitioners wishing to deploy this method should ad-
here to a strict validation protocol. This should include not
only statistical checks but also qualitative evaluation by do-
main experts. We strongly recommend the following di-
agnostics before trusting an ATE estimate in a real-world
application:
The naive model finds a large, statistically significant
harmful effect, suggesting pacemakers increase mortality
risk by 8.5 percentage points. This is likely due to con-
founding. In contrast, our JEPA-DML estimate is much
smaller and not statistically significant. This suggests that
once visual confounders related to patient health are ac-
counted for, the direct effect of the pacemaker is negligible,
a more clinically plausible result.
• Balance Checks: After estimating the confounder
proxy f(R), practitioners should check whether the
distributions of pre-treatment covariates are balanced
across treatment groups within strata of f(R).
6.2.1 Diagnostic Check 1: Covariate Balance
To validate that our learned confounder proxy f(R) is cap-
turing meaningful patient characteristics, we check for co-
variate balance. We use the proxy to stratify the population
• Placebo Tests: If possible, run the estimation proce-
dure using a ”placebo” treatment that is known to have
no causal effect. The estimated ATE should be close
to zero.
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
7
ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
• Sensitivity Analysis: Conduct a sensitivity analysis
(e.g., Rosenbaum-style bounds) to quantify how ro-
bust the estimated ATE is to potential violations of
the unconfoundedness assumption. This helps to un-
derstand the potential magnitude of bias from residual
confounding.
Failure to perform these checks could lead to the deploy-
ment of a biased model, which may perpetuate or even am-
plify existing societal inequities. The responsibility for the
ethical application of this method lies with the end-user,
who must ensure that the underlying assumptions are plau-
sible in their specific context.
8. Conclusion
This paper introduced a novel framework for causal in-
ference from high-dimensional data by integrating a self-
supervised predictive model (JEPA) with the statistical rigor
of Double Machine Learning. Our central hypothesis was
that the predictive objective of JEPA is particularly well-
suited to learning representations that capture latent con-
founding structures.
Our simulation study and real-world application pro-
vide significant support for our central claim. We demon-
strated that JEPA-DML outperforms methods based on non-
predictive SSL objectives, fine-tuned general-purpose mod-
els, and established end-to-end causal representation learn-
ing frameworks. Our diagnostics on the MIMIC-CXR
dataset further show the practical utility of our approach.
However, we acknowledge several important limitations
and address the reviewer’s concerns. First, as noted in W5,
our framework does not explicitly prevent the confounder
proxy f(R) from capturing mediators, which could induce
post-treatment bias. The current strategy of learning f(R)
to be predictive of the outcome may inadvertently select for
mediating variables. Procedural safeguards are a critical di-
rection for future work. For example, one could add an
adversarial loss during the training of f(R) to enforce in-
dependence between the proxy and the treatment T, or use
negative control outcomes known to be unaffected by the
treatment to detect and calibrate for mediator-induced bias.
Second, our empirical evidence, while now broader,
could be expanded further. We recommend future work
on stress-testing the framework by systematically varying
confounder signal strength and overlap, as well as explor-
ing proximal and sensitivity analyses for settings where the
sufficiency assumption is known to be violated.
Finally, our method relies on the strong, untestable As-
sumption 3.1. While our results provide evidence for its
plausibility in two distinct settings, real-world data will
rarely be so clean. Developing robust diagnostics to de-
tect residual confounding remains a critical open problem.
These limitations highlight the need for continued research
to bridge the gap between representation learning and trust-
worthy causal inference.
References
[1] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bo-
janowski, Armand Joulin, Nicolas Ballas, and Michael Rab-
bat. The vision-based joint embedding predictive architecture.
arXiv preprint arXiv:2402.00493, 2024. 1, 3
[2] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Es-
ther Duflo, Christian Hansen, Whitney Newey, and James
Robins. Double/debiased machine learning for treatment and
structural parameters. In The Econometrics Journal, vol-
ume 21, pages C1–C68. Wiley Online Library, 2018. 1, 4,
5
[3] Kosuke Imai, Kentaro Nakamura, and Luke Hollis. Genai-
powered causal inference. arXiv preprint arXiv:2402.02333,
2024. 1
[4] Alistair E. W. Johnson, Tom J. Pollard, Seth J. Berkowitz,
Nathaniel R. Greenbaum, Matthew P. Lungren, Chih-ying
Deng, Roger G. Mark, and Steven Horng. MIMIC-CXR, a
de-identified publicly available database of chest radiographs
with free-text reports. Scientific Data, 6(1):317, Dec. 2019. 7
[5] Bernhard Sch¨ olkopf, Francesco Locatello, Stefan Bauer,
Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and
Yoshua Bengio. Toward causal representation learning. Pro-
ceedings of the IEEE, 109(5):612–634, 2021. 1
[6] Uri Shalit, Fredrik D Johansson, and David Sontag. Estimat-
ing individual treatment effect: generalization bounds and al-
gorithms. In International conference on machine learning,
pages 3076–3085. PMLR, 2017. 4
[7] Yixin Wang and David M Blei. On the blessings of multi-
ple causes. Journal of the American Statistical Association,
114(528):1574–1592, 2019. 1
A. Appendix
A.1. A. Proofs and Derivations
A.1.1 Proof of Theorem 4.1
Proof. The goal is to show that the marginal distribution of
the potential outcome P(Yi(t,Ui) = y) can be identified
from the observed data distribution. We begin with the law
of iterated expectations:
P(Yi(t,Ui) = y) =
U
P(Yi(t,Ui) = y|Ui)dF(Ui)
(11)
By Assumption 2.2 (Unconfoundedness), we have
Yi(t,Ui) ⊥ Ti | Ui. This allows us to condition on the
observed treatment Ti = t:
=
U
P(Yi(t,Ui) = y|Ti = t,Ui)dF(Ui) (12)
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
8
ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
By consistency of potential outcomes (Yi = Yi(Ti,Ui)), we
can replace the potential outcome with the observed out-
come:
A.2. B. End-to-End Estimation Algorithm
Algorithm 1 End-to-End Causal Effect Estimation
=
U
P(Yi = y|Ti = t,Ui)dF(Ui) (13)
Now, we introduce the learned representation. By Assump-
tion 3.1 (Sufficiency of Representation), there exists a func-
tion f such that Ui = f(Ri). We can therefore substitute
f(Ri) for Ui and integrate over the distribution of Ri:
=
P(Yi = y|Ti = t,f(Ri))dF(Ri) R
(14)
1: Input: High-dimensional data {Xi}N
i=1, treatment as-
signments {Ti}N
i=1, outcomes {Yi}N
i=1, number of folds
K.
2: Output: ATE estimateˆ
τ and varianceˆ
σ2
.
3: // DML with Cross-Fitting
4: Partition the dataset indices {1,...,N}into K folds,
I1,...,IK.
5: Initialize an empty list for scores Ψ ←[].
6: for k= 1,...,Kdo
7: Define training indices Itrain = {1,...,N}\Ik.
8: Define test indices Itest = Ik.
9: // Stage 1: Fold-Specific Representation Learn-
ing
10: Train JEPA encoder E(k)
¯
θ on training data
{Xi}i∈Itrain.
11: Compute representations for all units in fold k:
R(k)
i = E(k)
¯
θ (Xi) for i∈Itrain ∪Itest.
12: // Stage 2: Fit nuisance models on training data
13: Jointly fit confounder proxyˆ
f(k) and nuisance
modelsˆ
µ(k)
ˆ
0 ,
µ(k)
ˆ
1 ,
π(k) on {(Yi,Ti,R(k)
i )}i∈Itrain. ▷All
models takeˆ
f(k)(R(k)
i ) as input.
14: // Predict on test data (out-of-sample)
15: for i∈Itest do
16:ˆ
fi ←ˆ
f(k)(R(k)
i )
17:ˆ
µ0,i ←ˆ
µ(k)
ˆ
0 (
fi)
18:ˆ
µ1,i ←ˆ
µ(k)
ˆ
1 (
fi)
19:ˆ
πi ←ˆ
π(k)(
ˆ
fi)
20: // Compute score for unit i
21: ψi ←Ti (Yi−
ˆ
µ1,i )
(1−Ti )(Yi−
ˆ
µ0,i )
−
ˆ
πi
1−
ˆ
πi
+ ˆ µ1,i−
ˆ
µ0,i
22: Append ψi to Ψ.
23: end for
24: end for
25: // Final ATE Estimation
26: Compute ATE estimate:ˆ
1
τ =
N
N
i=1 Ψi.
27: Compute variance:ˆ
σ2 =
1
N
N
i=1(Ψi−
ˆ
τ)2
.
28: returnˆ
τ,ˆ
σ2/N.
A.3. C. Reproducibility Details
In response to the reviewer’s request for complete repro-
ducibility details (W6), we provide an expanded description
of our experimental setup. All code and pre-trained models
will be made publicly available.
This gives the identification formula. The conditional in-
dependence Yi(t) ⊥ Ti | f(Ri) follows from the previ-
ous assumptions. Specifically, since Yi(t) is a function of
Ui, and Ui = f(Ri), the unconfoundedness assumption
Yi(t) ⊥Ti |Ui implies Yi(t) ⊥Ti |f(Ri). The positivity
condition (Assumption 2.3) ensures that the conditioning is
well-defined.
A.3.1 Simulation Details
The Colored MNIST simulation was run for 500 replica-
tions. Each replication used a sample size of N = 5,000.
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
9
ICCV
#****
ICCV
#****
ICCV 2026 Submission #****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
The data-generating process is as described in the main text.
JEPA and SSL Baselines: The ViT-B/16 encoder was
used for all from-scratch SSL models (JEPA, VICReg, Sim-
CLR). The representation dimension was dR = 768. All
models were trained for 100 epochs using the AdamW op-
timizer with a learning rate of 1e−4 and a cosine decay
schedule. For JEPA, we used a standard block masking
scheme with a ratio of 0.4. The confounder proxy network
f(R) was a 2-layer MLP with hidden dimension 128 and a
final output dimension of 32.
Nuisance Models: For all DML-based methods, the
nuisance functions (µ0,µ1,π) were estimated using Light-
GBM with default hyperparameters from the EconML li-
brary. We used K = 5 folds for cross-fitting.
Other Baselines: For DINOv2-FT, we used the official
ViT-B/14 model and fine-tuned it for 50 epochs on the sim-
ulation data. For CEVAE and DragonNet, we used the im-
plementations from the ‘causal-learn‘ library, with hyperpa-
rameters tuned via cross-validation on a held-out set.
A.3.2 MIMIC-CXR Application Details
The cohort from MIMIC-CXR consisted of 54,215 pa-
tients. The JEPA encoder (ViT-B/16) was pre-trained on all
377,110 images in the dataset for 200 epochs. The DML es-
timation procedure was then run on the labeled cohort data
using the same nuisance model setup as in the simulation.
The placebo treatment (PICC line) was identified in 8,124
patients. All analyses were conducted with institutional re-
view board approval.
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
10
